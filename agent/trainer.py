# agent/trainer.py

import json
import pickle
import numpy as np
from pathlib import Path
import random
import pandas as pd
from scipy.special import softmax


class QLearner:
    def __init__(self, epsilon=0.9, alpha=0.1, gamma=0.95,n_actions=2):
        self.n_actions = n_actions
        self.epsilon = epsilon
        self.alpha = alpha
        self.gamma = gamma
        self.q_table = {}  # key = state, value = [Q(0), Q(1)]

        # è¨“ç·´ç´€éŒ„
        self.total_reward = 0
        self.max_drawdown = 0
        self.roi = 0.0
        self.hit_rate = 0.0
        self.aborted = False  # ğŸ”¥ é‡è¦ï¼šè¨˜éŒ„æ˜¯å¦ä¸­æ­¢

    def _get_state(self, row):
        return (row.get('diff', 0), row.get('rolling_sum_5', 0))
    @staticmethod
    def calculate_entropy(q_values: np.ndarray) -> float:
        """æ ¹æ“š Q å€¼è¨ˆç®— softmax entropy"""
        probs = softmax(q_values)
        return -np.sum(probs * np.log(probs + 1e-8))  # é˜²æ­¢ log(0)


    def _choose_action(self, state):
        if state not in self.q_table:
            self.q_table[state] = [0.0] * self.n_actions

        q_values = self.q_table[state]
        if np.random.rand() < self.epsilon:
            return np.random.choice(range(self.n_actions))
        else:
            return int(np.argmax(q_values))

    def _update_q_value(self, state, action, reward, next_state):
        if next_state not in self.q_table:
            self.q_table[next_state] = [0.0] * self.n_actions
        predict = self.q_table[state][action]
        target = reward + self.gamma * max(self.q_table[next_state])
        self.q_table[state][action] += self.alpha * (target - predict)

    def _get_reward(self, row, action):
        wine_type = row.get('wine_type', 0)

        if action == 1:  # æŠ¼å°è»Š
            return 20 if wine_type == 1 else -80
        elif action == 2:  # æŠ¼å¤§è»Š
            return 120 if wine_type == 0 else -80
        else:  # ä¸ä¸‹æ³¨ / è§€æœ›
            return 0

    def train(self, df, episodes=1000, on_step=None, should_abort=None):
        self.aborted = False
        entropy_log = []
        self.q_table = {}

        total_hits = 0
        total_bets = 0
        max_cum_reward = 0
        n_data = len(df)
        MAX_FAIL = 5

        for ep in range(episodes):
            if should_abort and should_abort():
                self.aborted = True
                if on_step:
                    on_step("âš¡ è¨“ç·´ä¸­æ­¢ã€‚")
                break

            start_idx = random.randint(0, n_data - 1)
            step_limit = random.randint(10, 30)
            idx = start_idx
            steps = 0
            fail_count = 0
            cumulative_reward = 0
            row = df.iloc[idx]
            state = self._get_state(row)
            action = self._choose_action(state)
            reward = self._get_reward(row, action)
            next_idx = (idx + 1) % n_data
            next_row = df.iloc[next_idx]
            next_state = self._get_state(next_row)

            while steps < step_limit and fail_count < MAX_FAIL:
                row = df.iloc[idx]
                state = self._get_state(row)
                action = self._choose_action(state)
                reward = self._get_reward(row, action)
                next_idx = (idx + 1) % n_data
                next_row = df.iloc[next_idx]
                next_state = self._get_state(next_row)

                self._update_q_value(state, action, reward, next_state)

                cumulative_reward += reward
                total_bets += 1
                if reward > 0:
                    total_hits += 1
                else:
                    fail_count += 1

                max_cum_reward = max(max_cum_reward, cumulative_reward)
                self.max_drawdown = max(self.max_drawdown, max_cum_reward - cumulative_reward)

                idx = next_idx
                steps += 1
                # åœ¨ while çµæŸå¾Œè¨˜éŒ„ entropyï¼ˆä»£è¡¨æ•´è¼ªï¼‰
            q_values = np.array(self.q_table[state])
            entropy = QLearner.calculate_entropy(q_values)
            entropy_log.append({
                "round": ep,
                "state": str(state),
                "diff": float(state[0]),  # â¬…ï¸ æ‹†å‡ºä¸¦è½‰ float
                "rolling_sum_5": float(state[1]),  # â¬…ï¸ åŒä¸Š
                "entropy": entropy,
                "action": action,
                "reward": cumulative_reward,
                "q_values": list(q_values),
            })


        self.total_reward = cumulative_reward
        self.roi = cumulative_reward / total_bets if total_bets else 0
        self.hit_rate = total_hits / total_bets if total_bets else 0

        df_ = pd.DataFrame(entropy_log)
        Path("logs").mkdir(parents=True, exist_ok=True)
        df_.to_csv("logs/entropy_data.csv", index=False)
        print(f"[INFO] Entropy log saved: {len(df_)} entries.")


        if not self.aborted and on_step:
            on_step("âœ… è¨“ç·´å®Œæˆ")

    def save(self, path):
        path = Path(path)
        path.parent.mkdir(parents=True, exist_ok=True)
        with open(path, "wb") as f:
            pickle.dump(self.q_table, f)

    def load(self, path):
        with open(path, "rb") as f:
            self.q_table = pickle.load(f)

